{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/c6/19022633bc5e3430cb6e1f2b4bdc329081fc3bedec24f30b0735796f59f8/opencv_python-4.4.0.46-cp37-cp37m-macosx_10_13_x86_64.whl (52.4MB)\n",
      "\u001b[K     |████████████████████████████████| 52.4MB 8.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /Users/nobu/anaconda3/lib/python3.7/site-packages (from opencv-python) (1.19.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.4.0.46\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Illegal line #2\n",
      "\t\"(44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),\n",
      "\"\n",
      "\tin file \"/Users/nobu/.matplotlib/stylelib/nobu-style.mplstyle\"\n",
      "Illegal line #3\n",
      "\t\"(148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),\n",
      "\"\n",
      "\tin file \"/Users/nobu/.matplotlib/stylelib/nobu-style.mplstyle\"\n",
      "Illegal line #4\n",
      "\t\"(227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),\n",
      "\"\n",
      "\tin file \"/Users/nobu/.matplotlib/stylelib/nobu-style.mplstyle\"\n",
      "Illegal line #5\n",
      "\t\"(188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)])\n",
      "\"\n",
      "\tin file \"/Users/nobu/.matplotlib/stylelib/nobu-style.mplstyle\"\n",
      "Bad val \"cycler('color',[(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),\" on line #1\n",
      "\t\"axes.prop_cycle: cycler('color',[(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),\n",
      "\"\n",
      "\tin file \"/Users/nobu/.matplotlib/stylelib/nobu-style.mplstyle\"\n",
      "\tKey axes.prop_cycle: 'cycler('color',[(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),' is not a valid cycler construction: unexpected EOF while parsing (<string>, line 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np \n",
    "import cv2\n",
    "from keras.preprocessing import image \n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from skimage.segmentation import mark_boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_value(path, dim): \n",
    "    '''This function will read an image and convert to a specified version \n",
    "    and resize depending on which algorithm is being used. '''\n",
    "    img = image.load_img(path, target_size = dim)\n",
    "    img = image.img_to_array(img)\n",
    "    return img/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(img_paths, dim): \n",
    "    '''This fucntion takes a list of image paths and returns the np array \n",
    "    corresponding to each image.  It also takes the dim and whether edge is \n",
    "    specified in order to pass it to another function to apply these parameters.  \n",
    "    This function uses get_image_value to perform these operations'''\n",
    "    final_array = []\n",
    "    from tqdm import tqdm\n",
    "    for path in tqdm(img_paths):\n",
    "        img = get_image_value(path, dim)\n",
    "        final_array.append(img)\n",
    "    final_array = np.array(final_array)  \n",
    "    return final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2919/2919 [00:10<00:00, 265.60it/s]\n",
      "100%|██████████| 325/325 [00:01<00:00, 277.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Value Counts\n",
      "1    2218\n",
      "0     701\n",
      "dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Test Value Counts\n",
      "1    247\n",
      "0     78\n",
      "dtype: int64\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "X Train Shape\n",
      "(2919, 150, 150, 3)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "X Test Shape\n",
      "(325, 150, 150, 3)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "def get_tts():\n",
    "    '''This function will create a train test split'''  \n",
    "    DIM =  (150,150) \n",
    "    np.random.seed(10)        \n",
    "    gun_paths = [f'./Separated/FinalImages/Gun/{i}' for i in os.listdir('./Separated/FinalImages/Gun')] \n",
    "    gun_labels = [1 for i in range(len(gun_paths))]\n",
    "    #human_paths = [f'./Separated/FinalImages/Human/{i}' for i in os.listdir('./Separated/FinalImages/Human')] \n",
    "    #human_labels = [2 for i in range(len(human_paths))]    \n",
    "    others_paths = [f'./Separated/FinalImages/Others/{i}' for i in os.listdir('./Separated/FinalImages/Others')]\n",
    "    np.random.shuffle(others_paths)\n",
    "    others_paths = others_paths[:len(gun_paths)- 500]\n",
    "    others_labels = [0 for i in range(len(others_paths))]\n",
    "\n",
    "    np.random.shuffle(gun_paths)\n",
    "    gun_paths = gun_paths[:len(gun_paths)+150]\n",
    "    others_paths = others_paths[:len(others_paths)+150]\n",
    "\n",
    "    gun_labels = [1 for i in range(len(gun_paths))]\n",
    "    #human_labels = [2 for i in range(len(human_labels))]\n",
    "    others_labels = [0 for i in range(len(others_paths))]\n",
    "    #paths = gun_paths + human_paths + others_paths\n",
    "    paths = gun_paths + others_paths\n",
    "    labels = gun_labels + others_labels\n",
    "    #labels = gun_labels + human_labels + others_labels\n",
    "    x_train, x_test, y_train, y_test = train_test_split(paths, labels, stratify = labels, train_size = .90, random_state = 10)\n",
    "\n",
    "    new_x_train = get_img_array(x_train, DIM)\n",
    "    new_x_test = get_img_array(x_test, DIM)\n",
    "    \n",
    "    print('Train Value Counts')\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('Test Value Counts')\n",
    "    print(pd.Series(y_test).value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('X Train Shape')\n",
    "    print(new_x_train.shape)\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('X Test Shape')\n",
    "    print(new_x_test.shape)\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = to_categorical(y_test)\n",
    "    y_train = to_categorical(y_train)\n",
    "    tts = (new_x_train, new_x_test, y_train, y_test)\n",
    "    return tts\n",
    "\n",
    "x_train, x_test, y_train, y_test = get_tts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D, Dense, Dropout, Flatten \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model(dim = (150,150, 3)):\n",
    "    '''This function will create and compile a CNN given the input dimension'''\n",
    "    inp_shape = dim\n",
    "    act = 'relu'\n",
    "    drop = .25\n",
    "    kernal_reg = regularizers.l1(.001)\n",
    "    optimizer = Adam(lr = .0001)    \n",
    "    model = Sequential() \n",
    "    model.add(Conv2D(64, kernel_size=(3,3),activation=act, input_shape = inp_shape, \n",
    "                     kernel_regularizer = kernal_reg,\n",
    "                     kernel_initializer = 'he_uniform',  padding = 'same', name = 'Input_Layer'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),  strides = (3,3)))\n",
    "    model.add(Conv2D(64, (3, 3), activation=act, kernel_regularizer = kernal_reg, \n",
    "                     kernel_initializer = 'he_uniform',padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides = (3,3))) \n",
    "    model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, \n",
    "                     kernel_initializer = 'he_uniform',padding = 'same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, \n",
    "                     kernel_initializer = 'he_uniform',padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides = (3,3)))  \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(2, activation='sigmoid', name = 'Output_Layer'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    #model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "92/92 [==============================] - 43s 457ms/step - loss: 11.5079 - accuracy: 0.7850 - val_loss: 10.0921 - val_accuracy: 0.9077\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.09211, saving model to ModelWeights.h5\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - 42s 457ms/step - loss: 9.7774 - accuracy: 0.8979 - val_loss: 8.6829 - val_accuracy: 0.9600\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.09211 to 8.68294, saving model to ModelWeights.h5\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - 44s 475ms/step - loss: 8.4324 - accuracy: 0.9330 - val_loss: 7.5323 - val_accuracy: 0.9508\n",
      "\n",
      "Epoch 00003: val_loss improved from 8.68294 to 7.53226, saving model to ModelWeights.h5\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - 44s 473ms/step - loss: 7.2705 - accuracy: 0.9581 - val_loss: 6.5049 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00004: val_loss improved from 7.53226 to 6.50490, saving model to ModelWeights.h5\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - 43s 473ms/step - loss: 6.3039 - accuracy: 0.9672 - val_loss: 5.6379 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00005: val_loss improved from 6.50490 to 5.63790, saving model to ModelWeights.h5\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - 42s 460ms/step - loss: 5.4679 - accuracy: 0.9780 - val_loss: 4.9235 - val_accuracy: 0.9877\n",
      "\n",
      "Epoch 00006: val_loss improved from 5.63790 to 4.92347, saving model to ModelWeights.h5\n",
      "Epoch 7/50\n",
      "92/92 [==============================] - 43s 463ms/step - loss: 4.7915 - accuracy: 0.9763 - val_loss: 4.3682 - val_accuracy: 0.9692\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.92347 to 4.36825, saving model to ModelWeights.h5\n",
      "Epoch 8/50\n",
      "92/92 [==============================] - 43s 467ms/step - loss: 4.2377 - accuracy: 0.9807 - val_loss: 3.9006 - val_accuracy: 0.9815\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.36825 to 3.90058, saving model to ModelWeights.h5\n",
      "Epoch 9/50\n",
      "92/92 [==============================] - 43s 464ms/step - loss: 3.8205 - accuracy: 0.9779 - val_loss: 3.5343 - val_accuracy: 0.9846\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.90058 to 3.53425, saving model to ModelWeights.h5\n",
      "Epoch 10/50\n",
      "92/92 [==============================] - 42s 461ms/step - loss: 3.4773 - accuracy: 0.9830 - val_loss: 3.2687 - val_accuracy: 0.9877\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.53425 to 3.26874, saving model to ModelWeights.h5\n",
      "Epoch 11/50\n",
      "92/92 [==============================] - 43s 471ms/step - loss: 3.2082 - accuracy: 0.9805 - val_loss: 3.0788 - val_accuracy: 0.9600\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.26874 to 3.07881, saving model to ModelWeights.h5\n",
      "Epoch 12/50\n",
      "92/92 [==============================] - 43s 464ms/step - loss: 3.0113 - accuracy: 0.9754 - val_loss: 2.8373 - val_accuracy: 0.9846\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.07881 to 2.83727, saving model to ModelWeights.h5\n",
      "Epoch 13/50\n",
      "92/92 [==============================] - 46s 498ms/step - loss: 2.8315 - accuracy: 0.9756 - val_loss: 2.6916 - val_accuracy: 0.9846\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.83727 to 2.69157, saving model to ModelWeights.h5\n",
      "Epoch 14/50\n",
      "92/92 [==============================] - 50s 539ms/step - loss: 2.6627 - accuracy: 0.9859 - val_loss: 2.5546 - val_accuracy: 0.9877\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.69157 to 2.55463, saving model to ModelWeights.h5\n",
      "Epoch 15/50\n",
      "92/92 [==============================] - 42s 458ms/step - loss: 2.5338 - accuracy: 0.9879 - val_loss: 2.4248 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.55463 to 2.42483, saving model to ModelWeights.h5\n",
      "Epoch 16/50\n",
      "92/92 [==============================] - 45s 488ms/step - loss: 2.4131 - accuracy: 0.9851 - val_loss: 2.3277 - val_accuracy: 0.9815\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.42483 to 2.32766, saving model to ModelWeights.h5\n",
      "Epoch 17/50\n",
      "92/92 [==============================] - 45s 494ms/step - loss: 2.3097 - accuracy: 0.9797 - val_loss: 2.2057 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.32766 to 2.20572, saving model to ModelWeights.h5\n",
      "Epoch 18/50\n",
      "92/92 [==============================] - 46s 506ms/step - loss: 2.1972 - accuracy: 0.9869 - val_loss: 2.1127 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.20572 to 2.11266, saving model to ModelWeights.h5\n",
      "Epoch 19/50\n",
      "92/92 [==============================] - 51s 552ms/step - loss: 2.1118 - accuracy: 0.9867 - val_loss: 2.0290 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.11266 to 2.02902, saving model to ModelWeights.h5\n",
      "Epoch 20/50\n",
      "92/92 [==============================] - 48s 522ms/step - loss: 2.0190 - accuracy: 0.9907 - val_loss: 1.9474 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.02902 to 1.94743, saving model to ModelWeights.h5\n",
      "Epoch 21/50\n",
      "92/92 [==============================] - 48s 521ms/step - loss: 1.9394 - accuracy: 0.9907 - val_loss: 1.8796 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.94743 to 1.87963, saving model to ModelWeights.h5\n",
      "Epoch 22/50\n",
      "92/92 [==============================] - 46s 494ms/step - loss: 1.8696 - accuracy: 0.9889 - val_loss: 1.8643 - val_accuracy: 0.9662\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.87963 to 1.86431, saving model to ModelWeights.h5\n",
      "Epoch 23/50\n",
      "92/92 [==============================] - 46s 505ms/step - loss: 1.8394 - accuracy: 0.9807 - val_loss: 1.7552 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.86431 to 1.75521, saving model to ModelWeights.h5\n",
      "Epoch 24/50\n",
      "92/92 [==============================] - 50s 540ms/step - loss: 1.7418 - accuracy: 0.9904 - val_loss: 1.7209 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.75521 to 1.72095, saving model to ModelWeights.h5\n",
      "Epoch 25/50\n",
      "92/92 [==============================] - 46s 494ms/step - loss: 1.6968 - accuracy: 0.9880 - val_loss: 1.6273 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.72095 to 1.62734, saving model to ModelWeights.h5\n",
      "Epoch 26/50\n",
      "92/92 [==============================] - 45s 486ms/step - loss: 1.6253 - accuracy: 0.9904 - val_loss: 1.5877 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.62734 to 1.58770, saving model to ModelWeights.h5\n",
      "Epoch 27/50\n",
      "92/92 [==============================] - 44s 481ms/step - loss: 1.5746 - accuracy: 0.9935 - val_loss: 1.5351 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.58770 to 1.53506, saving model to ModelWeights.h5\n",
      "Epoch 28/50\n",
      "92/92 [==============================] - 43s 472ms/step - loss: 1.5275 - accuracy: 0.9902 - val_loss: 1.4951 - val_accuracy: 0.9969\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.53506 to 1.49513, saving model to ModelWeights.h5\n",
      "Epoch 29/50\n",
      "92/92 [==============================] - 47s 514ms/step - loss: 1.4994 - accuracy: 0.9849 - val_loss: 1.4509 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.49513 to 1.45088, saving model to ModelWeights.h5\n",
      "Epoch 30/50\n",
      "92/92 [==============================] - 46s 500ms/step - loss: 1.4449 - accuracy: 0.9897 - val_loss: 1.4003 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.45088 to 1.40033, saving model to ModelWeights.h5\n",
      "Epoch 31/50\n",
      "92/92 [==============================] - 45s 487ms/step - loss: 1.4009 - accuracy: 0.9912 - val_loss: 1.3600 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.40033 to 1.36000, saving model to ModelWeights.h5\n",
      "Epoch 32/50\n",
      "92/92 [==============================] - 47s 505ms/step - loss: 1.3735 - accuracy: 0.9876 - val_loss: 1.3278 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.36000 to 1.32779, saving model to ModelWeights.h5\n",
      "Epoch 33/50\n",
      "92/92 [==============================] - 47s 511ms/step - loss: 1.3448 - accuracy: 0.9877 - val_loss: 1.3199 - val_accuracy: 0.9785\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.32779 to 1.31989, saving model to ModelWeights.h5\n",
      "Epoch 34/50\n",
      "92/92 [==============================] - 50s 540ms/step - loss: 1.3167 - accuracy: 0.9844 - val_loss: 1.2577 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00034: val_loss improved from 1.31989 to 1.25769, saving model to ModelWeights.h5\n",
      "Epoch 35/50\n",
      "92/92 [==============================] - 46s 500ms/step - loss: 1.2522 - accuracy: 0.9925 - val_loss: 1.2273 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.25769 to 1.22732, saving model to ModelWeights.h5\n",
      "Epoch 36/50\n",
      "92/92 [==============================] - 47s 509ms/step - loss: 1.2289 - accuracy: 0.9915 - val_loss: 1.2039 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00036: val_loss improved from 1.22732 to 1.20386, saving model to ModelWeights.h5\n",
      "Epoch 37/50\n",
      "92/92 [==============================] - 49s 535ms/step - loss: 1.2021 - accuracy: 0.9904 - val_loss: 1.1768 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00037: val_loss improved from 1.20386 to 1.17676, saving model to ModelWeights.h5\n",
      "Epoch 38/50\n",
      "92/92 [==============================] - 48s 527ms/step - loss: 1.1672 - accuracy: 0.9959 - val_loss: 1.1468 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.17676 to 1.14679, saving model to ModelWeights.h5\n",
      "Epoch 39/50\n",
      "92/92 [==============================] - 47s 516ms/step - loss: 1.1439 - accuracy: 0.9937 - val_loss: 1.1511 - val_accuracy: 0.9877\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.14679\n",
      "Epoch 40/50\n",
      "92/92 [==============================] - 46s 504ms/step - loss: 1.1557 - accuracy: 0.9779 - val_loss: 1.1102 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00040: val_loss improved from 1.14679 to 1.11017, saving model to ModelWeights.h5\n",
      "Epoch 41/50\n",
      "92/92 [==============================] - 45s 484ms/step - loss: 1.0974 - accuracy: 0.9939 - val_loss: 1.0760 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.11017 to 1.07603, saving model to ModelWeights.h5\n",
      "Epoch 42/50\n",
      "92/92 [==============================] - 50s 542ms/step - loss: 1.0698 - accuracy: 0.9965 - val_loss: 1.0496 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00042: val_loss improved from 1.07603 to 1.04957, saving model to ModelWeights.h5\n",
      "Epoch 43/50\n",
      "92/92 [==============================] - 48s 519ms/step - loss: 1.0649 - accuracy: 0.9881 - val_loss: 1.0371 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00043: val_loss improved from 1.04957 to 1.03714, saving model to ModelWeights.h5\n",
      "Epoch 44/50\n",
      "92/92 [==============================] - 46s 500ms/step - loss: 1.0474 - accuracy: 0.9884 - val_loss: 1.0210 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00044: val_loss improved from 1.03714 to 1.02097, saving model to ModelWeights.h5\n",
      "Epoch 45/50\n",
      "92/92 [==============================] - 47s 510ms/step - loss: 1.0090 - accuracy: 0.9948 - val_loss: 0.9980 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.02097 to 0.99797, saving model to ModelWeights.h5\n",
      "Epoch 46/50\n",
      "92/92 [==============================] - 45s 491ms/step - loss: 0.9952 - accuracy: 0.9910 - val_loss: 0.9961 - val_accuracy: 0.9877\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.99797 to 0.99613, saving model to ModelWeights.h5\n",
      "Epoch 47/50\n",
      "92/92 [==============================] - 48s 521ms/step - loss: 0.9810 - accuracy: 0.9943 - val_loss: 0.9568 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.99613 to 0.95679, saving model to ModelWeights.h5\n",
      "Epoch 48/50\n",
      "92/92 [==============================] - 45s 495ms/step - loss: 0.9596 - accuracy: 0.9938 - val_loss: 0.9466 - val_accuracy: 0.9908\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.95679 to 0.94660, saving model to ModelWeights.h5\n",
      "Epoch 49/50\n",
      "92/92 [==============================] - 46s 500ms/step - loss: 0.9311 - accuracy: 0.9976 - val_loss: 0.9380 - val_accuracy: 0.9877\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.94660 to 0.93805, saving model to ModelWeights.h5\n",
      "Epoch 50/50\n",
      "92/92 [==============================] - 46s 495ms/step - loss: 0.9446 - accuracy: 0.9881 - val_loss: 0.9131 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.93805 to 0.91310, saving model to ModelWeights.h5\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', verbose = 1, patience=10, min_delta = .00075)\n",
    "model_checkpoint = ModelCheckpoint('ModelWeights.h5', verbose = 1, save_best_only=True,\n",
    "                                  monitor = 'val_loss')\n",
    "lr_plat = ReduceLROnPlateau(patience = 2, mode = 'min')\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "model = get_conv_model()\n",
    "model_history = model.fit(x_train, y_train, batch_size = batch_size,\n",
    "            epochs = epochs, \n",
    "     callbacks = [early_stopping, model_checkpoint, lr_plat], validation_data = (x_test, y_test), verbose= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nobu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "/Users/nobu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:1397: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: model_save/assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "save_model(model, 'model_save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('ModelWeights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-17fab3a743ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model/my_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "model.save_model('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg/189.jpg\t\tPrediction: No Weapon\t99% Confident\n",
      "Creating Bounding Boxes for neg/189.jpg\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "def non_max_suppression(boxes, overlapThresh= .5):\n",
    "    '''This image was taken from PyImageSearch... again cannot thank that guy enough'''\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "    # initialize the list of picked indexes\t\n",
    "    pick = []\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1, y1, x2, y2 = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]    \n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        # find the largest (x, y) coordinates for the start of\n",
    "        # the bounding box and the smallest (x, y) coordinates\n",
    "        # for the end of the bounding box\n",
    "        xx1, yy1, xx2, yy2 = np.maximum(x1[i], x1[idxs[:last]]), np.maximum(y1[i], y1[idxs[:last]]), np.minimum(x2[i], x2[idxs[:last]]), np.minimum(y2[i], y2[idxs[:last]])\n",
    "        # compute the width and height of the bounding box\n",
    "        w, h = np.maximum(0, xx2 - xx1 + 1), np.maximum(0, yy2 - yy1 + 1)\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    "    # return only the bounding boxes that were picked using the\n",
    "    # integer data type\n",
    "    return pick\n",
    "\n",
    "def get_img_prediction_bounding_box(path, model, dim):\n",
    "    '''This function will create a bounding box over what it believes \n",
    "    is a weapon given the image path, dimensions, and model used to \n",
    "    detect the weapon.  Dimensions can be found within the Var.py file.  \n",
    "    This function is still being used as I need to apply non-max suppresion \n",
    "    to create only one bounding box'''\n",
    "    img = get_image_value(path, dim)   \n",
    "    img = img.reshape(1, img.shape[0], img.shape[1], 3)\n",
    "    pred = model.predict(img)[0]\n",
    "    category_dict = {0: 'No Weapon', 1: 'Gun', 2: 'Human'}\n",
    "    cat_index = np.argmax(pred)\n",
    "    cat = category_dict[cat_index]\n",
    "    print(f'{path}\\t\\tPrediction: {cat}\\t{int(pred.max()*100)}% Confident')\n",
    "\n",
    "    #speed up cv2\n",
    "    cv2.setUseOptimized(True)\n",
    "    cv2.setNumThreads(10) #change depending on your computer\n",
    "    img = cv2.imread(path)\n",
    "    clone = img.copy() \n",
    "    clone2 = img.copy()\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(img)\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "\n",
    "    rects = ss.process() \n",
    "    windows = []\n",
    "    locations = []\n",
    "    print(f'Creating Bounding Boxes for {path}')\n",
    "    for x, y, w,h in rects[:1001]: \n",
    "        startx, starty, endx, endy = x, y, x+w, y+h \n",
    "        roi = img[starty:endy, startx:endx]\n",
    "        roi = cv2.resize(roi, dsize =dim, interpolation = cv2.INTER_CUBIC)\n",
    "        windows.append(roi)\n",
    "        locations.append((startx, starty, endx, endy))\n",
    "    windows = np.array(windows)\n",
    "    windows = windows.reshape(windows.shape[0], windows.shape[1], windows.shape[2], 3)\n",
    "    windows = np.array(windows)\n",
    "    locations = np.array(locations)\n",
    "    predictions = model.predict(windows)\n",
    "    nms = non_max_suppression(locations)\n",
    "    bounding_cnt = 0\n",
    "    for idx in nms:\n",
    "        if np.argmax(predictions[idx]) != cat_index: \n",
    "            continue\n",
    "        startx, starty, endx, endy = locations[idx]\n",
    "        cv2.rectangle(clone, (startx, starty), (endx, endy), (0,0,255), 2)\n",
    "        text = f'{category_dict[np.argmax(predictions[idx])]}: {int(predictions[idx].max()*100)}%'\n",
    "        cv2.putText(clone, text, (startx, starty+15), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,255,0),2)\n",
    "        bounding_cnt += 1\n",
    "\n",
    "    if bounding_cnt == 0: \n",
    "        pred_idx= [idx for idx, i in enumerate(predictions) if np.argmax(i) == cat_index]\n",
    "        cat_locations = np.array([locations[i] for i in pred_idx])\n",
    "        nms = non_max_suppression(cat_locations)\n",
    "        if len(nms)==0:\n",
    "            cat_predictions = predictions[:,cat_index]\n",
    "            pred_max_idx = np.argmax(cat_predictions)\n",
    "            pred_max = cat_predictions[pred_max_idx]\n",
    "            pred_max_window = locations[pred_max_idx]\n",
    "            startx, starty, endx, endy = pred_max_window\n",
    "            cv2.rectangle(clone, (startx, starty), (endx, endy),  (0,0,255),2)\n",
    "            text = f'{category_dict[cat_index]}: {int(pred_max*100)}%'\n",
    "            cv2.putText(clone, text, (startx, starty+15), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,255,0),2)\n",
    "        for idx in nms: \n",
    "            startx, starty, endx, endy = cat_locations[idx]\n",
    "            cv2.rectangle(clone, (startx, starty), (endx, endy), (0,0,255), 2)\n",
    "            text = f'{category_dict[np.argmax(predictions[pred_idx[idx]])]}: {int(predictions[pred_idx[idx]].max()*100)}%'\n",
    "            cv2.putText(clone, text, (startx, starty+15), cv2.FONT_HERSHEY_SIMPLEX, .5, (0,255,0),2)        \n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    cv2.imshow(f'Test', np.hstack([clone, clone2]))\n",
    "    cv2.waitKey(0)\n",
    "    ss.clear()\n",
    "    return predictions\n",
    "\n",
    "#NORMAL MODEL\n",
    "dim = (150, 150, 3)    \n",
    "normal_model = get_conv_model(dim)\n",
    "normal_model.load_weights('ModelWeights.h5') #path to the model weights\n",
    "test_folder = 'neg' #folder where you will put your images to test\n",
    "predictions = []\n",
    "for idx, i in enumerate([i for i in os.listdir(test_folder) if i != 'ipynb_checkpoints']):\n",
    "    img_path = f'{test_folder}/{i}'\n",
    "    pred = get_img_prediction_bounding_box(img_path, normal_model, dim = (150,150))\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_model = get_conv_model(dim)\n",
    "normal_model.load_weights('ModelWeights.h5') #path to the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'Output_Layer')>]\n",
      "[<KerasTensor: shape=(None, 150, 150, 3) dtype=float32 (created by layer 'Input_Layer_input')>]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.makedirs('./model', exist_ok=True)\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('ModelWeights.h5')\n",
    "print(model.outputs)\n",
    "# [<tf.Tensor 'dense_2/Softmax:0' shape=(?, 10) dtype=float32>]\n",
    "print(model.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "pre_model = tf.keras.models.load_model(\"ModelWeights.h5\")\n",
    "pre_model.save(\"saved_model\")\n",
    "#tf.keras.models.save_model(\"ModelWeights.h5\", 'saved_model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retval = cv.dnn.readNetFromTensorflow('saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.io.gfile' has no attribute 'FastGFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8227b2bffda6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Read the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model/saved_model.pb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.io.gfile' has no attribute 'FastGFile'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "\n",
    "# Read the graph.\n",
    "with tf.io.gfile.FastGFile('saved_model/saved_model.pb', 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore session\n",
    "    sess.graph.as_default()\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    # Read and preprocess an image.\n",
    "    img = cv.imread('neg/332.jpg')\n",
    "    rows = img.shape[0]\n",
    "    cols = img.shape[1]\n",
    "    inp = cv.resize(img, (300, 300))\n",
    "    inp = inp[:, :, [2, 1, 0]]  # BGR2RGB\n",
    "\n",
    "    # Run the model\n",
    "    out = sess.run([sess.graph.get_tensor_by_name('num_detections:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_scores:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_boxes:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_classes:0')],\n",
    "                   feed_dict={'image_tensor:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\n",
    "\n",
    "    # Visualize detected bounding boxes.\n",
    "    num_detections = int(out[0][0])\n",
    "    for i in range(num_detections):\n",
    "        classId = int(out[3][0][i])\n",
    "        score = float(out[1][0][i])\n",
    "        bbox = [float(v) for v in out[2][0][i]]\n",
    "        if score > 0.3:\n",
    "            x = bbox[1] * cols\n",
    "            y = bbox[0] * rows\n",
    "            right = bbox[3] * cols\n",
    "            bottom = bbox[2] * rows\n",
    "            cv.rectangle(img, (int(x), int(y)), (int(right), int(bottom)), (125, 255, 51), thickness=2)\n",
    "\n",
    "cv.imshow('TensorFlow MobileNet-SSD', img)\n",
    "cv.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "DecodeError",
     "evalue": "Error parsing message",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDecodeError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2674e6bc382f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'frozen_graph.pbtxt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDecodeError\u001b[0m: Error parsing message"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "graph_filename = \"saved_model/saved_model.pb\"\n",
    "with tf.compat.v2.io.gfile.GFile(graph_filename, \"rb\")as f:\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    tf.train.write_graph(graph_def, './', 'frozen_graph.pbtxt', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0-rc0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ModelWeights.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b66680687043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_constants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_variables_to_constants_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ModelWeights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0minfer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'serving_default'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m   \"\"\"\n\u001b[0;32m--> 859\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m   saved_model_proto, debug_info = (\n\u001b[0;32m--> 871\u001b[0;31m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   debug_info_path = os.path.join(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ModelWeights.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "loaded = tf.saved_model.load('ModelWeights.h5')\n",
    "infer = loaded.signatures['serving_default']\n",
    "f = tf.function(infer).get_concrete_function(flatten_input=tf.TensorSpec(shape=[None, 28, 28, 1], dtype=tf.float32))\n",
    "f2 = convert_variables_to_constants_v2(f)\n",
    "graph_def = f2.graph.as_graph_def()\n",
    "# Export frozen graph\n",
    "with tf.io.gfile.GFile('frozen_graph.pb', 'wb') as f:\n",
    "   f.write(graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e4e92a383ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreeze_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ModelWeights.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Saved_model2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-0d92ab84a915>\u001b[0m in \u001b[0;36mfreeze_session\u001b[0;34m(session, keep_var_names, output_names, clear_devices)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0;32mreturn\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfrozen\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdefinition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mfreeze_var_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_var_names\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "freeze_session(session='ModelWeights.h5', output_names='Saved_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'gfile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b316a059fc34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Read the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFastGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved_model/saved_model.pb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'gfile'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "\n",
    "# Read the graph.\n",
    "with tf.gfile.FastGFile('saved_model/saved_model.pb', 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore session\n",
    "    sess.graph.as_default()\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    # Read and preprocess an image.\n",
    "    img = cv.imread('img_85.jpg')\n",
    "    rows = img.shape[0]\n",
    "    cols = img.shape[1]\n",
    "    inp = cv.resize(img, (300, 300))\n",
    "    inp = inp[:, :, [2, 1, 0]]  # BGR2RGB\n",
    "\n",
    "    # Run the model\n",
    "    out = sess.run([sess.graph.get_tensor_by_name('num_detections:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_scores:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_boxes:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_classes:0')],\n",
    "                   feed_dict={'image_tensor:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\n",
    "\n",
    "    # Visualize detected bounding boxes.\n",
    "    num_detections = int(out[0][0])\n",
    "    for i in range(num_detections):\n",
    "        classId = int(out[3][0][i])\n",
    "        score = float(out[1][0][i])\n",
    "        bbox = [float(v) for v in out[2][0][i]]\n",
    "        if score > 0.3:\n",
    "            x = bbox[1] * cols\n",
    "            y = bbox[0] * rows\n",
    "            right = bbox[3] * cols\n",
    "            bottom = bbox[2] * rows\n",
    "            cv.rectangle(img, (int(x), int(y)), (int(right), int(bottom)), (125, 255, 51), thickness=2)\n",
    "\n",
    "cv.imshow('TensorFlow MobileNet-SSD', img)\n",
    "cv.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
